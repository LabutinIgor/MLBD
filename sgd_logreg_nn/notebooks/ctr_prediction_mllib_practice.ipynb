{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#CTR-prediction\" data-toc-modified-id=\"CTR-prediction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CTR-prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Formulation\" data-toc-modified-id=\"Problem-Formulation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Formulation</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-construction:\" data-toc-modified-id=\"Dataset-construction:-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Dataset construction:</a></span></li><li><span><a href=\"#Format:\" data-toc-modified-id=\"Format:-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Format:</a></span></li></ul></li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Metrics</a></span></li></ul></li><li><span><a href=\"#Dataset-preprocessing\" data-toc-modified-id=\"Dataset-preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#ML-Pipelines-(Transformers,-Estimators)\" data-toc-modified-id=\"ML-Pipelines-(Transformers,-Estimators)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components\" target=\"_blank\">ML Pipelines (Transformers, Estimators)</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-stages-of-pipeline\" data-toc-modified-id=\"Prepare-stages-of-pipeline-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Prepare stages of pipeline</a></span></li><li><span><a href=\"#Fit-and-save-pipeline\" data-toc-modified-id=\"Fit-and-save-pipeline-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Fit and save pipeline</a></span></li><li><span><a href=\"#Load-fitted-pipeline\" data-toc-modified-id=\"Load-fitted-pipeline-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Load fitted pipeline</a></span></li><li><span><a href=\"#Transform-dataset-using-pipeline\" data-toc-modified-id=\"Transform-dataset-using-pipeline-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Transform dataset using pipeline</a></span></li><li><span><a href=\"#Make-dataset-split\" data-toc-modified-id=\"Make-dataset-split-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Make dataset split</a></span></li></ul></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html\" target=\"_blank\">Classification</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\" target=\"_blank\">Logistic Regression</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-and-Train-model\" data-toc-modified-id=\"Define-and-Train-model-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Define and Train model</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html\" target=\"_blank\">Evaluation</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-classification-metrics\" data-toc-modified-id=\"Binary-classification-metrics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#binary-classification\" target=\"_blank\">Binary classification metrics</a></a></span></li><li><span><a href=\"#Make-submission\" data-toc-modified-id=\"Make-submission-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Make submission</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR-prediction\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "$\\newcommand{\\vecw}{{\\bf w}}$\n",
    "$\\newcommand{\\vecx}{{\\bf x}}$\n",
    "\n",
    "* Dataset: $X^N = \\{ z_i \\}^N_{i=1}$, где $z_i = (\\vecx_i, y_i) \\sim P(z), y_i \\in \\{0,1\\}$\n",
    "* Prediction: $$ \\hat{y}_i = f_{\\vecw}(\\vecx_i) =  \\mathbb{P} \\left\\{ y = 1 \\mid \\vecx_i \\right\\} $$\n",
    "* Loss function (Binary Cross-Entropy): $$ \\min\\limits_{\\vecw} \\quad \\frac{\\lambda}{2}\\| \\vecw \\|^2_2 - \\frac{1}{N} \\sum\\limits_{i=1}^{N} y_i \\log \\hat{y}_i + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "\n",
    "## Dataset\n",
    "$ $\n",
    "<details>\n",
    "  <summary>Click here to see the details</summary>\n",
    "\n",
    "For more details see `/data/criteo/readme.txt`\n",
    "\n",
    "### Dataset construction:\n",
    "\n",
    "\n",
    ">There are 13 features taking **integer** values and 26\n",
    "**categorical** features. The values of the categorical features have been hashed\n",
    "onto 32 bits for anonymization purposes. \n",
    "Some features may have missing values.\n",
    "\n",
    "> The rows are chronologically ordered by `id` column.\n",
    "\n",
    "> The test set corresponds to events on the day following the training period. \n",
    "The first column (`label`) has been removed.\n",
    "\n",
    "\n",
    "### Format:\n",
    "\n",
    "> The columns are comma separeted with the following schema:\n",
    "`<label>,<integer feature 1>, ... <integer feature 13>,<categorical feature 1>, ... <categorical feature 26>,<id>`\n",
    "\n",
    "> When a value is missing, the field is \"\". There is no `label` field in the test set.\n",
    "\n",
    "</details>\n",
    "    \n",
    "## Metrics\n",
    "\n",
    "The evaluation metrics for this task are\n",
    "* ROC AUC\n",
    "* LogLoss\n",
    "* [Normalized Entropy](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/workspace/data/criteo'\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin our introduction to Spark [MLlib](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset preprocessing\n",
    "\n",
    "Before we can train any prediction model on our dataset we need to conver each row into real-valued features vector ($\\vecx \\in \\mathbb{R}^n$).\n",
    "\n",
    "Spark MLlib provides easy to use tools for preprocessing raw features and turning them into suitable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only first two categorical features for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns = ['_c{}'.format(i) for i in range(1, 14)]\n",
    "cat_columns = ['_c{}'.format(i) for i in range(14, 40)][:2]\n",
    "len(num_columns), len(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ML Pipelines (Transformers, Estimators)](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components)\n",
    "\n",
    "\n",
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n",
    "\n",
    "* `Transformer`: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "\n",
    "* `Estimator`: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "\n",
    "* `Pipeline`: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "---\n",
    "Basically speaking `transformer` is an instance of class that implements `transform` method, and both `estimator` and `pipeline` implements `transform` and `fit` methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stages of pipeline\n",
    "\n",
    "We might benefit from using `StringIndexer, OneHotEncoderEstimator, VectorAssembler` (see [doc](https://spark.apache.org/docs/latest/ml-features) for details) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 1833503\n",
      "    _c0  _c1   _c2  _c3  _c4    _c5  _c6  _c7  _c8  _c9  ...      _c31  \\\n",
      "0     1    0    -1    0    0   1465    0   17    0    4  ...  e5f8f18f   \n",
      "1     1    0     1   20   16   1548   93   42   32  912  ...  1f868fdd   \n",
      "2     0    0    53    0   10   6550   98   34   11  349  ...  fa0643ee   \n",
      "3     0    0     7    1    0   9128    0    0    0    7  ...  d6be9f7b   \n",
      "4     0    0   179    4    4  17232    0    0   18    7  ...  2804effd   \n",
      "5     0    0    18   15    9   4494    0    0    9    8  ...  e4ca448c   \n",
      "6     1    1     5   33    6    375   74    3   46  258  ...  04d863d5   \n",
      "7     0    0    19   31    8   7130  126    1   20  116  ...  df4fffb7   \n",
      "8     0    0     8    0    2  30617    0    0    5   40  ...  281769c2   \n",
      "9     0    0  2242   13    7  11786   63   29   13   87  ...  bc48b783   \n",
      "10    0    0     1    0    1      0    0    0    1    1  ...  19fec6cc   \n",
      "11    0    0     0    0    3   1992    5    6    5    5  ...  e7e991cb   \n",
      "12    1    0     0    3    6   8223  123    5   10  130  ...  86b4c7aa   \n",
      "13    1    0    -1    0    0   8020   26    6    0   80  ...  7119e567   \n",
      "14    0    1   359    1   22    154   22   12    1   59  ...  97b81540   \n",
      "15    0    0     0   12    6   3100  247   24   12  246  ...  b486119d   \n",
      "16    0    1   165    8    1      2    0    2    2   10  ...  5aed7436   \n",
      "17    1    0   116    0    0   6600  175   22   21  214  ...  bf6b118a   \n",
      "18    0    0   370    1    3   1887   15    1   14   14  ...  2efa89c6   \n",
      "19    0    0     2   14    0  37566  160    1    0  158  ...  8f0f692f   \n",
      "\n",
      "        _c32      _c33      _c34      _c35      _c36      _c37      _c38  \\\n",
      "0       None      None  f3ddd519      None  32c7478e  b34f3128      None   \n",
      "1   21ddcdc9  a458ea53  7eee76d1      None  32c7478e  9af06ad9  9d93af03   \n",
      "2   21ddcdc9  b1252a9d  0094bc78      None  32c7478e  29ece3ed  001f3601   \n",
      "3       None      None  64e9eec3      None  be7c41b4  1793a828      None   \n",
      "4       None      None  723b4dfd      None  c7dc6720  b34f3128      None   \n",
      "5       None      None  f973405d      None  3a171ecb  9117a34a      None   \n",
      "6   54e53a39  b1252a9d  f1e1f2b3      None  423fab69  45ab94c8  e8b83407   \n",
      "7   21ddcdc9  5840adea  09f172ad      None  3a171ecb  c8a524a7  010f6491   \n",
      "8       None      None  dfcfc3fa      None  3a171ecb  aee52b6f      None   \n",
      "9       None      None  0014c32a  ad3062eb  55dd3565  3b183c5c      None   \n",
      "10  21ddcdc9  a458ea53  38d30bf3      None  32c7478e  1370c56e  001f3601   \n",
      "11  21ddcdc9  a458ea53  f0db6227      None  32c7478e  1f163fc7  ea9a246c   \n",
      "12      None      None  71d4501c      None  32c7478e  b34f3128      None   \n",
      "13  1d04f4a4  b1252a9d  d5f54153      None  32c7478e  a9d771cd  c9f3bea7   \n",
      "14      None      None  0ac4575d      None  3a171ecb  d28d80ac      None   \n",
      "15      None      None  8bf2a88b      None  32c7478e  0691aeff      None   \n",
      "16  c79aad78  5840adea  18907521  c9d4222a  32c7478e  af931d1c  e8b83407   \n",
      "17  21ddcdc9  b1252a9d  78766d37      None  32c7478e  9e0bee34  445bbe3b   \n",
      "18      None      None  73d06dde      None  93bad2c0  aee52b6f      None   \n",
      "19  21ddcdc9  b1252a9d  cc6a9262  8ec974f4  32c7478e  a5862ce8  e8b83407   \n",
      "\n",
      "        _c39   id  \n",
      "0       None   12  \n",
      "1   cdfe5ab7   26  \n",
      "2   402185f3   85  \n",
      "3       None  139  \n",
      "4       None  151  \n",
      "5       None  175  \n",
      "6   c84c4aec  212  \n",
      "7   4e7af834  252  \n",
      "8       None  255  \n",
      "9       None  268  \n",
      "10  c27f155b  348  \n",
      "11  8c532d04  376  \n",
      "12      None  382  \n",
      "13  0a47000d  405  \n",
      "14      None  419  \n",
      "15      None  424  \n",
      "16  60404332  440  \n",
      "17  3246a38c  455  \n",
      "18      None  462  \n",
      "19  c8816bd2  515  \n",
      "\n",
      "[20 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"count\", df.count())\n",
    "print(df.limit(20).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "\n",
    "indexersTF = [StringIndexer(inputCol=column, outputCol=column+'_index').setHandleInvalid(\"keep\").fit(df) for column in cat_columns]\n",
    "\n",
    "cat_columns_index = list(map(lambda x : x + '_index', cat_columns))\n",
    "cat_columns_vecs = list(map(lambda x: x + '_vec', cat_columns))\n",
    "oneHotTF = OneHotEncoderEstimator(inputCols=cat_columns_index, outputCols=cat_columns_vecs)\n",
    "vectorAssemblerTF = VectorAssembler(inputCols=num_columns + cat_columns_vecs, outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexersTF + [oneHotTF, vectorAssemblerTF])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and save pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_model = pipeline.fit(df)\n",
    "\n",
    "# transform_model.save('transform_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fitted pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "transform_model_loaded = PipelineModel.load('transform_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             features  label   id\n",
      "0   (0.0, -1.0, 0.0, 0.0, 1465.0, 0.0, 17.0, 0.0, ...      1   12\n",
      "1   (0.0, 1.0, 20.0, 16.0, 1548.0, 93.0, 42.0, 32....      1   26\n",
      "2   (0.0, 53.0, 0.0, 10.0, 6550.0, 98.0, 34.0, 11....      0   85\n",
      "3   (0.0, 7.0, 1.0, 0.0, 9128.0, 0.0, 0.0, 0.0, 7....      0  139\n",
      "4   (0.0, 179.0, 4.0, 4.0, 17232.0, 0.0, 0.0, 18.0...      0  151\n",
      "5   (0.0, 18.0, 15.0, 9.0, 4494.0, 0.0, 0.0, 9.0, ...      0  175\n",
      "6   (1.0, 5.0, 33.0, 6.0, 375.0, 74.0, 3.0, 46.0, ...      1  212\n",
      "7   (0.0, 19.0, 31.0, 8.0, 7130.0, 126.0, 1.0, 20....      0  252\n",
      "8   (0.0, 8.0, 0.0, 2.0, 30617.0, 0.0, 0.0, 5.0, 4...      0  255\n",
      "9   (0.0, 2242.0, 13.0, 7.0, 11786.0, 63.0, 29.0, ...      0  268\n",
      "10  (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, ...      0  348\n",
      "11  (0.0, 0.0, 0.0, 3.0, 1992.0, 5.0, 6.0, 5.0, 5....      0  376\n",
      "12  (0.0, 0.0, 3.0, 6.0, 8223.0, 123.0, 5.0, 10.0,...      1  382\n",
      "13  (0.0, -1.0, 0.0, 0.0, 8020.0, 26.0, 6.0, 0.0, ...      1  405\n",
      "14  (1.0, 359.0, 1.0, 22.0, 154.0, 22.0, 12.0, 1.0...      0  419\n",
      "15  (0.0, 0.0, 12.0, 6.0, 3100.0, 247.0, 24.0, 12....      0  424\n",
      "16  (1.0, 165.0, 8.0, 1.0, 2.0, 0.0, 2.0, 2.0, 10....      0  440\n",
      "17  (0.0, 116.0, 0.0, 0.0, 6600.0, 175.0, 22.0, 21...      1  455\n",
      "18  (0.0, 370.0, 1.0, 3.0, 1887.0, 15.0, 1.0, 14.0...      0  462\n",
      "19  (0.0, 2.0, 14.0, 0.0, 37566.0, 160.0, 1.0, 0.0...      0  515\n"
     ]
    }
   ],
   "source": [
    "df_transformed = transform_model \\\n",
    "    .transform(df) \\\n",
    "    .withColumnRenamed('_c0', 'label') \\\n",
    "    .select('features', 'label', 'id')\n",
    "\n",
    "print(df_transformed.limit(20).toPandas())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset split\n",
    "\n",
    "Spark provides [randomSplit](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method.\n",
    "\n",
    "It is not the best choice in our task since we have chronological order in data.\n",
    "\n",
    "We need to implement our own split function which will split the data in parts with respect to chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "def split_by_col(df, split_col, parts_fractions):\n",
    "    \"\"\"\n",
    "    df - DataFrame\n",
    "    split_col - total order column\n",
    "    parts_fractions - fractions of resulting parts\n",
    "    \"\"\"\n",
    "    \n",
    "    split_col_window = Window.orderBy(split_col)\n",
    "\n",
    "    df_percent_ranks = df.withColumn('rank', F.percent_rank().over(split_col_window))\n",
    "    \n",
    "    parts = []\n",
    "    start_fraction = 0\n",
    "    for part_fraction in parts_fractions:\n",
    "        part = df_percent_ranks \\\n",
    "            .filter(F.col('rank') >= start_fraction) \\\n",
    "            .filter(F.col('rank') < start_fraction + part_fraction) \\\n",
    "            .drop('rank')\n",
    "        parts.append(part)\n",
    "        start_fraction += part_fraction\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_by_col(df_transformed, 'id', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999997818383717, 0.09999983637877877, 0.09999983637877877)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() / N, val_df.count() / N, test_df.count() / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Classification](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "## [Logistic Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)\n",
    "\n",
    "### Define and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(regParam=0.3)\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(1958, {0: 5.0, 1: 156.0, 3: 3.0, 4: 210.0, 5: 6.0, 6: 5.0, 7: 5.0, 8: 6.0, 9: 1.0, 10: 1.0, 12: 3.0, 14: 1.0, 1404: 1.0}), label=0, id=455266596465, rawPrediction=DenseVector([0.7734, -0.7734]), probability=DenseVector([0.6842, 0.3158]), prediction=0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.transform(val_df).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Evaluation](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)\n",
    "\n",
    "## [Binary classification metrics](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#binary-classification)\n",
    "\n",
    "* ROC AUC\n",
    "* LogLoss\n",
    "* Normalized Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "def rocauc(model, df):\n",
    "    pred = model.transform(df)\n",
    "    predictionAndLabels = pred.rdd.map(lambda x: (float(x.probability[1]), float(x.label)))\n",
    "    metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    return metrics.areaUnderROC\n",
    "\n",
    "\n",
    "def logloss(model, df):\n",
    "    pred = model.transform(df)\n",
    "\n",
    "    firstelementUdf = udf(lambda v:float(v[1]), FloatType())\n",
    "    df_logloss = pred \\\n",
    "        .withColumn(\n",
    "            'logloss'\n",
    "            , -F.col('label') * F.log(firstelementUdf('probability')) - (1. - F.col('label')) * F.log(1. - (firstelementUdf('probability')))\n",
    "        )\n",
    "\n",
    "    res_value = float(df_logloss \\\n",
    "        .agg(F.mean('logloss')) \\\n",
    "        .head()[0])\n",
    "\n",
    "\n",
    "    return res_value\n",
    "\n",
    "\n",
    "def ne(model, df):\n",
    "    ll = logloss(model, df)\n",
    "    p = df.agg(F.avg(F.col('label'))).head()[0]\n",
    "    const_entropy = -p * math.log(p) - (1. - p) * math.log(1. - p)\n",
    "    return ll / const_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6991721429752401"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(lr_model, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9452191045321535"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne(lr_model, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7008679800202761"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(lr_model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9445117066566621"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne(lr_model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission\n",
    "\n",
    "Join the [competition](https://www.kaggle.com/c/mlbd-20-ctr-prediction-1) and make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = os.path.join(DATA_PATH, 'test.csv')\n",
    "\n",
    "df_test = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TEST_PATH)\n",
    "\n",
    "df_test.printSchema()\n",
    "\n",
    "df_test = df_test.fillna(0, subset=num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             features            id\n",
      "0   (0.0, 19.0, 2.0, 4.0, 4576.0, 6.0, 6.0, 5.0, 1...  566935904713\n",
      "1   (0.0, 1.0, 1.0, 0.0, 5688.0, 0.0, 0.0, 2.0, 10...  566935904715\n",
      "2   (0.0, 445.0, 2.0, 2.0, 8579.0, 26.0, 1.0, 2.0,...  566935904727\n",
      "3   (0.0, 172.0, 7.0, 1.0, 2008.0, 143.0, 24.0, 28...  566935904737\n",
      "4   (0.0, 11.0, 4.0, 4.0, 14.0, 0.0, 0.0, 4.0, 6.0...  566935904741\n",
      "5   (1.0, 0.0, 19.0, 27.0, 108.0, 33.0, 14.0, 33.0...  566935904745\n",
      "6   (7.0, 2.0, 2.0, 6.0, 296.0, 13.0, 27.0, 9.0, 2...  566935904752\n",
      "7   (0.0, 0.0, 210.0, 46.0, 16102.0, 0.0, 0.0, 46....  566935904759\n",
      "8   (0.0, 2.0, 1.0, 1.0, 17621.0, 40.0, 3.0, 5.0, ...  566935904767\n",
      "9   (0.0, 56.0, 0.0, 0.0, 9794.0, 0.0, 0.0, 30.0, ...  566935904768\n",
      "10  (0.0, 12.0, 1.0, 3.0, 6081.0, 69.0, 8.0, 6.0, ...  566935904778\n",
      "11  (4.0, 3.0, 9.0, 11.0, 1229.0, 53.0, 4.0, 43.0,...  566935904785\n",
      "12  (0.0, 1124.0, 0.0, 9.0, 2756.0, 13.0, 42.0, 11...  566935904789\n",
      "13  (1.0, 0.0, 4.0, 1.0, 1284.0, 13.0, 1.0, 10.0, ...  566935904804\n",
      "14  (10.0, 77.0, 1.0, 2.0, 959.0, 28.0, 66.0, 12.0...  566935904810\n",
      "15  (0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  566935904817\n",
      "16  (0.0, 115.0, 2.0, 1.0, 1380.0, 39.0, 12.0, 13....  566935904819\n",
      "17  (0.0, -1.0, 0.0, 0.0, 4495.0, 13.0, 2.0, 0.0, ...  566935904830\n",
      "18  (0.0, 0.0, 7.0, 1.0, 6878.0, 21.0, 9.0, 18.0, ...  566935904847\n",
      "19  (0.0, 123.0, 0.0, 0.0, 24454.0, 513.0, 8.0, 0....  566935904848\n"
     ]
    }
   ],
   "source": [
    "df_test_transformed = transform_model \\\n",
    "    .transform(df_test) \\\n",
    "    .select('features', 'id') \\\n",
    "\n",
    "print(df_test_transformed.limit(20).toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=566935904713, proba=0.24282529950141907), Row(id=566935904715, proba=0.2642812728881836), Row(id=566935904727, proba=0.26935726404190063), Row(id=566935904737, proba=0.35397103428840637), Row(id=566935904741, proba=0.25624197721481323), Row(id=566935904745, proba=0.2616666853427887), Row(id=566935904752, proba=0.3720690608024597), Row(id=566935904759, proba=0.15140342712402344), Row(id=566935904767, proba=0.3080241084098816), Row(id=566935904768, proba=0.21472062170505524)]\n",
      "+------------+----------+\n",
      "|          id|     proba|\n",
      "+------------+----------+\n",
      "|566935904713| 0.2428253|\n",
      "|566935904715|0.26428127|\n",
      "|566935904727|0.26935726|\n",
      "|566935904737|0.35397103|\n",
      "|566935904741|0.25624198|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstelementUdf = udf(lambda v:float(v[1]), FloatType())\n",
    "\n",
    "pred_test_df = lr_model \\\n",
    "        .transform(df_test_transformed) \\\n",
    "        .select('id', 'probability') \\\n",
    "        .withColumn('proba', firstelementUdf('probability')) \\\n",
    "        .drop('probability')\n",
    "\n",
    "print(pred_test_df.take(10))\n",
    "pred_test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PREDICTIONS_PATH = 'test_predictions'\n",
    "\n",
    "pred_test_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('/workspace/data/' + TEST_PREDICTIONS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
