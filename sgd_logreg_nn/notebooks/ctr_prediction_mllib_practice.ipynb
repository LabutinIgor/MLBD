{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#CTR-prediction\" data-toc-modified-id=\"CTR-prediction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CTR-prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Formulation\" data-toc-modified-id=\"Problem-Formulation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Formulation</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-construction:\" data-toc-modified-id=\"Dataset-construction:-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Dataset construction:</a></span></li><li><span><a href=\"#Format:\" data-toc-modified-id=\"Format:-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Format:</a></span></li></ul></li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Metrics</a></span></li></ul></li><li><span><a href=\"#Dataset-preprocessing\" data-toc-modified-id=\"Dataset-preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#ML-Pipelines-(Transformers,-Estimators)\" data-toc-modified-id=\"ML-Pipelines-(Transformers,-Estimators)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components\" target=\"_blank\">ML Pipelines (Transformers, Estimators)</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-stages-of-pipeline\" data-toc-modified-id=\"Prepare-stages-of-pipeline-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Prepare stages of pipeline</a></span></li><li><span><a href=\"#Fit-and-save-pipeline\" data-toc-modified-id=\"Fit-and-save-pipeline-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Fit and save pipeline</a></span></li><li><span><a href=\"#Load-fitted-pipeline\" data-toc-modified-id=\"Load-fitted-pipeline-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Load fitted pipeline</a></span></li><li><span><a href=\"#Transform-dataset-using-pipeline\" data-toc-modified-id=\"Transform-dataset-using-pipeline-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Transform dataset using pipeline</a></span></li><li><span><a href=\"#Make-dataset-split\" data-toc-modified-id=\"Make-dataset-split-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Make dataset split</a></span></li></ul></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html\" target=\"_blank\">Classification</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\" target=\"_blank\">Logistic Regression</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-and-Train-model\" data-toc-modified-id=\"Define-and-Train-model-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Define and Train model</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html\" target=\"_blank\">Evaluation</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-classification-metrics\" data-toc-modified-id=\"Binary-classification-metrics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#binary-classification\" target=\"_blank\">Binary classification metrics</a></a></span></li><li><span><a href=\"#Make-submission\" data-toc-modified-id=\"Make-submission-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Make submission</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR-prediction\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "$\\newcommand{\\vecw}{{\\bf w}}$\n",
    "$\\newcommand{\\vecx}{{\\bf x}}$\n",
    "\n",
    "* Dataset: $X^N = \\{ z_i \\}^N_{i=1}$, где $z_i = (\\vecx_i, y_i) \\sim P(z), y_i \\in \\{0,1\\}$\n",
    "* Prediction: $$ \\hat{y}_i = f_{\\vecw}(\\vecx_i) =  \\mathbb{P} \\left\\{ y = 1 \\mid \\vecx_i \\right\\} $$\n",
    "* Loss function (Binary Cross-Entropy): $$ \\min\\limits_{\\vecw} \\quad \\frac{\\lambda}{2}\\| \\vecw \\|^2_2 - \\frac{1}{N} \\sum\\limits_{i=1}^{N} y_i \\log \\hat{y}_i + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "\n",
    "## Dataset\n",
    "$ $\n",
    "<details>\n",
    "  <summary>Click here to see the details</summary>\n",
    "\n",
    "For more details see `/data/criteo/readme.txt`\n",
    "\n",
    "### Dataset construction:\n",
    "\n",
    "\n",
    ">There are 13 features taking **integer** values and 26\n",
    "**categorical** features. The values of the categorical features have been hashed\n",
    "onto 32 bits for anonymization purposes. \n",
    "Some features may have missing values.\n",
    "\n",
    "> The rows are chronologically ordered by `id` column.\n",
    "\n",
    "> The test set corresponds to events on the day following the training period. \n",
    "The first column (`label`) has been removed.\n",
    "\n",
    "\n",
    "### Format:\n",
    "\n",
    "> The columns are comma separeted with the following schema:\n",
    "`<label>,<integer feature 1>, ... <integer feature 13>,<categorical feature 1>, ... <categorical feature 26>,<id>`\n",
    "\n",
    "> When a value is missing, the field is \"\". There is no `label` field in the test set.\n",
    "\n",
    "</details>\n",
    "    \n",
    "## Metrics\n",
    "\n",
    "The evaluation metrics for this task are\n",
    "* ROC AUC\n",
    "* LogLoss\n",
    "* [Normalized Entropy](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/workspace/data/criteo'\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin our introduction to Spark [MLlib](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset preprocessing\n",
    "\n",
    "Before we can train any prediction model on our dataset we need to conver each row into real-valued features vector ($\\vecx \\in \\mathbb{R}^n$).\n",
    "\n",
    "Spark MLlib provides easy to use tools for preprocessing raw features and turning them into suitable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only first two categorical features for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns = ['_c{}'.format(i) for i in range(1, 14)]\n",
    "cat_columns = ['_c{}'.format(i) for i in range(14, 40)][:2]\n",
    "len(num_columns), len(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ML Pipelines (Transformers, Estimators)](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components)\n",
    "\n",
    "\n",
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n",
    "\n",
    "* `Transformer`: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "\n",
    "* `Estimator`: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "\n",
    "* `Pipeline`: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "---\n",
    "Basically speaking `transformer` is an instance of class that implements `transform` method, and both `estimator` and `pipeline` implements `transform` and `fit` methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stages of pipeline\n",
    "\n",
    "We might benefit from using `StringIndexer, OneHotEncoderEstimator, VectorAssembler` (see [doc](https://spark.apache.org/docs/latest/ml-features) for details) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 1831624\n",
      "    _c0  _c1   _c2  _c3  _c4    _c5   _c6  _c7  _c8   _c9  ...      _c31  \\\n",
      "0     1    0    -1    0    0   1465     0   17    0     4  ...  e5f8f18f   \n",
      "1     1    0     1   20   16   1548    93   42   32   912  ...  1f868fdd   \n",
      "2     0    8     0   15   20    115    24    8   23    24  ...  1304f63b   \n",
      "3     1    0    -1    0    0    446    30    2   27    28  ...  fffe2a63   \n",
      "4     1    1     0   31    9    226    22   32   13   177  ...  1f868fdd   \n",
      "5     0    0     7    1    0   9128     0    0    0     7  ...  d6be9f7b   \n",
      "6     0    0   179    4    4  17232     0    0   18     7  ...  2804effd   \n",
      "7     0    0    19   32    0   1994     0    0   19    26  ...  582152eb   \n",
      "8     0    0    76    3    0   5029     0    0   16     0  ...  2804effd   \n",
      "9     0    0     1   51   11  20692  1645    3   21  1580  ...  5aed7436   \n",
      "10    1    2    30   27    2      0     0    2    2     2  ...  8e8b535e   \n",
      "11    0    0  2242   13    7  11786    63   29   13    87  ...  bc48b783   \n",
      "12    1    0     0    1    0      0     0    0    1     0  ...  f03e8b05   \n",
      "13    0    0    -1    0    0      0     0    0    4    39  ...  b133fcd4   \n",
      "14    0    0     1    0    1      0     0    0    1     1  ...  19fec6cc   \n",
      "15    0    0     0    0    3   1992     5    6    5     5  ...  e7e991cb   \n",
      "16    1    0    -1    0    0   8020    26    6    0    80  ...  7119e567   \n",
      "17    1    1    71    2    4    419     7  303   35   609  ...  63cdbb21   \n",
      "18    0    0    13   15    5  10471     0    0    5    55  ...  bc48b783   \n",
      "19    0    1   359    1   22    154    22   12    1    59  ...  97b81540   \n",
      "\n",
      "        _c32      _c33      _c34      _c35      _c36      _c37      _c38  \\\n",
      "0       None      None  f3ddd519      None  32c7478e  b34f3128      None   \n",
      "1   21ddcdc9  a458ea53  7eee76d1      None  32c7478e  9af06ad9  9d93af03   \n",
      "2   21ddcdc9  b1252a9d  07b2853e      None  32c7478e  94bde4f2  010f6491   \n",
      "3   21ddcdc9  b1252a9d  eb0fc6f8      None  32c7478e  df487a73  001f3601   \n",
      "4   cd11c728  a458ea53  00680113      None  32c7478e  ad4c56a0  e8b83407   \n",
      "5       None      None  64e9eec3      None  be7c41b4  1793a828      None   \n",
      "6       None      None  723b4dfd      None  c7dc6720  b34f3128      None   \n",
      "7   21ddcdc9  5840adea  fbaf98df      None  32c7478e  e773f0cb  001f3601   \n",
      "8       None      None  723b4dfd      None  3a171ecb  b34f3128      None   \n",
      "9   21ddcdc9  a458ea53  86dec00a      None  3a171ecb  c62cc6c2  e8b83407   \n",
      "10  21ddcdc9  b1252a9d  11da5050  ad3062eb  32c7478e  9d214089  ea9a246c   \n",
      "11      None      None  0014c32a  ad3062eb  55dd3565  3b183c5c      None   \n",
      "12      None      None  950d91c1      None  55dd3565  2f7e98de      None   \n",
      "13      None      None  2b796e4a  ad3062eb  32c7478e  8d365d3b      None   \n",
      "14  21ddcdc9  a458ea53  38d30bf3      None  32c7478e  1370c56e  001f3601   \n",
      "15  21ddcdc9  a458ea53  f0db6227      None  32c7478e  1f163fc7  ea9a246c   \n",
      "16  1d04f4a4  b1252a9d  d5f54153      None  32c7478e  a9d771cd  c9f3bea7   \n",
      "17  cf99e5de  5840adea  5f957280      None  55dd3565  1793a828  e8b83407   \n",
      "18      None      None  0014c32a      None  32c7478e  3b183c5c      None   \n",
      "19      None      None  0ac4575d      None  3a171ecb  d28d80ac      None   \n",
      "\n",
      "        _c39   id  \n",
      "0       None   12  \n",
      "1   cdfe5ab7   26  \n",
      "2   09b76f8d   39  \n",
      "3   c27f155b  117  \n",
      "4   8270b5de  121  \n",
      "5       None  139  \n",
      "6       None  151  \n",
      "7   1b0ebd59  199  \n",
      "8       None  206  \n",
      "9   33358f02  235  \n",
      "10  5d3f5a67  249  \n",
      "11      None  268  \n",
      "12      None  302  \n",
      "13      None  313  \n",
      "14  c27f155b  348  \n",
      "15  8c532d04  376  \n",
      "16  0a47000d  405  \n",
      "17  b7d9c3bc  413  \n",
      "18      None  416  \n",
      "19      None  419  \n",
      "\n",
      "[20 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"count\", df.count())\n",
    "print(df.limit(20).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "\n",
    "indexersTF = [StringIndexer(inputCol=column, outputCol=column+'_index').setHandleInvalid(\"keep\").fit(df) for column in cat_columns]\n",
    "\n",
    "cat_columns_index = list(map(lambda x : x + '_index', cat_columns))\n",
    "cat_columns_vecs = list(map(lambda x: x + '_vec', cat_columns))\n",
    "oneHotTF = OneHotEncoderEstimator(inputCols=cat_columns_index, outputCols=cat_columns_vecs)\n",
    "vectorAssemblerTF = VectorAssembler(inputCols=num_columns + cat_columns_vecs, outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexersTF + [oneHotTF, vectorAssemblerTF])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and save pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_model = pipeline.fit(df)\n",
    "\n",
    "transform_model.save(os.path.join(DATA_PATH, 'pipeline_model'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fitted pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "transform_model_loaded = PipelineModel.load(os.path.join(DATA_PATH, 'pipeline_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             features  label   id\n",
      "0   (0.0, -1.0, 0.0, 0.0, 1465.0, 0.0, 17.0, 0.0, ...      1   12\n",
      "1   (0.0, 1.0, 20.0, 16.0, 1548.0, 93.0, 42.0, 32....      1   26\n",
      "2   (8.0, 0.0, 15.0, 20.0, 115.0, 24.0, 8.0, 23.0,...      0   39\n",
      "3   (0.0, -1.0, 0.0, 0.0, 446.0, 30.0, 2.0, 27.0, ...      1  117\n",
      "4   (1.0, 0.0, 31.0, 9.0, 226.0, 22.0, 32.0, 13.0,...      1  121\n",
      "5   (0.0, 7.0, 1.0, 0.0, 9128.0, 0.0, 0.0, 0.0, 7....      0  139\n",
      "6   (0.0, 179.0, 4.0, 4.0, 17232.0, 0.0, 0.0, 18.0...      0  151\n",
      "7   (0.0, 19.0, 32.0, 0.0, 1994.0, 0.0, 0.0, 19.0,...      0  199\n",
      "8   (0.0, 76.0, 3.0, 0.0, 5029.0, 0.0, 0.0, 16.0, ...      0  206\n",
      "9   (0.0, 1.0, 51.0, 11.0, 20692.0, 1645.0, 3.0, 2...      0  235\n",
      "10  (2.0, 30.0, 27.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0...      1  249\n",
      "11  (0.0, 2242.0, 13.0, 7.0, 11786.0, 63.0, 29.0, ...      0  268\n",
      "12  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...      1  302\n",
      "13  (0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 39.0...      0  313\n",
      "14  (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, ...      0  348\n",
      "15  (0.0, 0.0, 0.0, 3.0, 1992.0, 5.0, 6.0, 5.0, 5....      0  376\n",
      "16  (0.0, -1.0, 0.0, 0.0, 8020.0, 26.0, 6.0, 0.0, ...      1  405\n",
      "17  (1.0, 71.0, 2.0, 4.0, 419.0, 7.0, 303.0, 35.0,...      1  413\n",
      "18  (0.0, 13.0, 15.0, 5.0, 10471.0, 0.0, 0.0, 5.0,...      0  416\n",
      "19  (1.0, 359.0, 1.0, 22.0, 154.0, 22.0, 12.0, 1.0...      0  419\n"
     ]
    }
   ],
   "source": [
    "df_transformed = transform_model \\\n",
    "    .transform(df) \\\n",
    "    .withColumnRenamed('_c0', 'label') \\\n",
    "    .select('features', 'label', 'id')\n",
    "\n",
    "print(df_transformed.limit(20).toPandas())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset split\n",
    "\n",
    "Spark provides [randomSplit](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method.\n",
    "\n",
    "It is not the best choice in our task since we have chronological order in data.\n",
    "\n",
    "We need to implement our own split function which will split the data in parts with respect to chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "def split_by_col(df, split_col, parts_fractions):\n",
    "    \"\"\"\n",
    "    df - DataFrame\n",
    "    split_col - total order column\n",
    "    parts_fractions - fractions of resulting parts\n",
    "    \"\"\"\n",
    "    \n",
    "    split_col_window = Window.orderBy(split_col)\n",
    "\n",
    "    df_percent_ranks = df.withColumn('rank', F.percent_rank().over(split_col_window))\n",
    "    \n",
    "    parts = []\n",
    "    start_fraction = 0\n",
    "    for part_fraction in parts_fractions:\n",
    "        part = df_percent_ranks \\\n",
    "            .filter(F.col('rank') >= start_fraction) \\\n",
    "            .filter(F.col('rank') < start_fraction + part_fraction) \\\n",
    "            .drop('rank')\n",
    "        parts.append(part)\n",
    "        start_fraction += part_fraction\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_by_col(df_transformed, 'id', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999998908072836, 0.09999978161456718, 0.09999978161456718)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() / N, val_df.count() / N, test_df.count() / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Classification](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "## [Logistic Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)\n",
    "\n",
    "### Define and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(regParam=0.3)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "lr_model.save(os.path.join(DATA_PATH, 'lr_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(1968, {2: 6.0, 4: 401427.0, 7: 1.0, 8: 2.0, 13: 1.0, 1433: 1.0}), label=0, id=455266582849, rawPrediction=DenseVector([1.6698, -1.6698]), probability=DenseVector([0.8415, 0.1585]), prediction=0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.transform(val_df).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Evaluation](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)\n",
    "\n",
    "## [Binary classification metrics](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#binary-classification)\n",
    "\n",
    "* ROC AUC\n",
    "* LogLoss\n",
    "* Normalized Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "def rocauc(model, df):\n",
    "    pred = model.transform(df)\n",
    "    predictionAndLabels = pred.rdd.map(lambda x: (float(x.probability[1]), float(x.label)))\n",
    "    metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    return metrics.areaUnderROC\n",
    "\n",
    "\n",
    "def logloss(model, df):\n",
    "    pred = model.transform(df)\n",
    "\n",
    "    firstelementUdf = udf(lambda v:float(v[1]), FloatType())\n",
    "    df_logloss = pred \\\n",
    "        .withColumn(\n",
    "            'logloss'\n",
    "            , -F.col('label') * F.log(firstelementUdf('probability')) - (1. - F.col('label')) * F.log(1. - (firstelementUdf('probability')))\n",
    "        )\n",
    "\n",
    "    res_value = float(df_logloss \\\n",
    "        .agg(F.mean('logloss')) \\\n",
    "        .head()[0])\n",
    "\n",
    "\n",
    "    return res_value\n",
    "\n",
    "\n",
    "def ne(model, df):\n",
    "    ll = logloss(model, df)\n",
    "    p = df.agg(F.avg(F.col('label'))).head()[0]\n",
    "    const_entropy = -p * math.log(p) - (1. - p) * math.log(1. - p)\n",
    "    return ll / const_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6990836501877037"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(lr_model, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462895656173077"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne(lr_model, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6981911784211167"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(lr_model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9469062251250209"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne(lr_model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission\n",
    "\n",
    "Join the [competition](https://www.kaggle.com/c/mlbd-20-ctr-prediction-1) and make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = os.path.join(DATA_PATH, 'test.csv')\n",
    "\n",
    "df_test = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TEST_PATH)\n",
    "\n",
    "df_test.printSchema()\n",
    "\n",
    "df_test = df_test.fillna(0, subset=num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             features            id\n",
      "0   (0.0, 19.0, 2.0, 4.0, 4576.0, 6.0, 6.0, 5.0, 1...  566935904713\n",
      "1   (0.0, 1.0, 1.0, 0.0, 5688.0, 0.0, 0.0, 2.0, 10...  566935904715\n",
      "2   (0.0, 445.0, 2.0, 2.0, 8579.0, 26.0, 1.0, 2.0,...  566935904727\n",
      "3   (0.0, 172.0, 7.0, 1.0, 2008.0, 143.0, 24.0, 28...  566935904737\n",
      "4   (0.0, 11.0, 4.0, 4.0, 14.0, 0.0, 0.0, 4.0, 6.0...  566935904741\n",
      "5   (1.0, 0.0, 19.0, 27.0, 108.0, 33.0, 14.0, 33.0...  566935904745\n",
      "6   (7.0, 2.0, 2.0, 6.0, 296.0, 13.0, 27.0, 9.0, 2...  566935904752\n",
      "7   (0.0, 0.0, 210.0, 46.0, 16102.0, 0.0, 0.0, 46....  566935904759\n",
      "8   (0.0, 2.0, 1.0, 1.0, 17621.0, 40.0, 3.0, 5.0, ...  566935904767\n",
      "9   (0.0, 56.0, 0.0, 0.0, 9794.0, 0.0, 0.0, 30.0, ...  566935904768\n",
      "10  (0.0, 12.0, 1.0, 3.0, 6081.0, 69.0, 8.0, 6.0, ...  566935904778\n",
      "11  (4.0, 3.0, 9.0, 11.0, 1229.0, 53.0, 4.0, 43.0,...  566935904785\n",
      "12  (0.0, 1124.0, 0.0, 9.0, 2756.0, 13.0, 42.0, 11...  566935904789\n",
      "13  (1.0, 0.0, 4.0, 1.0, 1284.0, 13.0, 1.0, 10.0, ...  566935904804\n",
      "14  (10.0, 77.0, 1.0, 2.0, 959.0, 28.0, 66.0, 12.0...  566935904810\n",
      "15  (0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  566935904817\n",
      "16  (0.0, 115.0, 2.0, 1.0, 1380.0, 39.0, 12.0, 13....  566935904819\n",
      "17  (0.0, -1.0, 0.0, 0.0, 4495.0, 13.0, 2.0, 0.0, ...  566935904830\n",
      "18  (0.0, 0.0, 7.0, 1.0, 6878.0, 21.0, 9.0, 18.0, ...  566935904847\n",
      "19  (0.0, 123.0, 0.0, 0.0, 24454.0, 513.0, 8.0, 0....  566935904848\n"
     ]
    }
   ],
   "source": [
    "df_test_transformed = transform_model \\\n",
    "    .transform(df_test) \\\n",
    "    .select('features', 'id') \\\n",
    "\n",
    "print(df_test_transformed.limit(20).toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=566935904713, proba=0.24410083889961243), Row(id=566935904715, proba=0.2607521414756775), Row(id=566935904727, proba=0.26791608333587646), Row(id=566935904737, proba=0.34679165482521057), Row(id=566935904741, proba=0.2531881630420685), Row(id=566935904745, proba=0.2713431119918823), Row(id=566935904752, proba=0.38257014751434326), Row(id=566935904759, proba=0.16029205918312073), Row(id=566935904767, proba=0.30799728631973267), Row(id=566935904768, proba=0.2126682549715042)]\n",
      "+------------+----------+\n",
      "|          id|     proba|\n",
      "+------------+----------+\n",
      "|566935904713|0.24410084|\n",
      "|566935904715|0.26075214|\n",
      "|566935904727|0.26791608|\n",
      "|566935904737|0.34679165|\n",
      "|566935904741|0.25318816|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstelementUdf = udf(lambda v:float(v[1]), FloatType())\n",
    "\n",
    "pred_test_df = lr_model \\\n",
    "        .transform(df_test_transformed) \\\n",
    "        .select('id', 'probability') \\\n",
    "        .withColumn('proba', firstelementUdf('probability')) \\\n",
    "        .drop('probability')\n",
    "\n",
    "print(pred_test_df.take(10))\n",
    "pred_test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PREDICTIONS_PATH = 'test_predictions'\n",
    "\n",
    "pred_test_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('/workspace/data/' + TEST_PREDICTIONS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
